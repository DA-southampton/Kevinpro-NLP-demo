{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Train GPT on addition\n",
    "\n",
    "Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# set up logging\r\n",
    "import logging\r\n",
    "logging.basicConfig(\r\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\r\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
    "        level=logging.INFO,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# make deterministic\r\n",
    "from mingpt.utils import set_seed\r\n",
    "set_seed(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from torch.utils.data import Dataset\r\n",
    "\r\n",
    "class AdditionDataset(Dataset):\r\n",
    "    \"\"\"\r\n",
    "    Returns addition problems of up to some number of digits in the inputs. Recall\r\n",
    "    that all GPT cares about are sequences of integers, and completing them according to\r\n",
    "    patterns in the data. Therefore, we have to somehow encode addition problems\r\n",
    "    as a sequence of integers.\r\n",
    "    \r\n",
    "    The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\r\n",
    "    encoding will simply be the n-digit first number, n-digit second number, \r\n",
    "    and (n+1)-digit result, all simply concatenated together. Because each addition\r\n",
    "    problem is so structured, there is no need to bother the model with encoding\r\n",
    "    +, =, or other tokens. Each possible sequence has the same length, and simply\r\n",
    "    contains the raw digits of the addition problem.\r\n",
    "    \r\n",
    "    As a few examples, the 2-digit problems:\r\n",
    "    - 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\r\n",
    "    - 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\r\n",
    "    etc.\r\n",
    "    \r\n",
    "    We will also only train GPT on the final (n+1)-digits because the first\r\n",
    "    two n-digits are always assumed to be given. So when we give GPT an exam later,\r\n",
    "    we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\r\n",
    "    to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5]\r\n",
    "    in 3 sequential steps.\r\n",
    "    \r\n",
    "    fun exercise: does it help if the result is asked to be produced in reverse order?\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, ndigit, split):\r\n",
    "        self.split = split # train/test\r\n",
    "        self.ndigit = ndigit\r\n",
    "        self.vocab_size = 10 # 10 possible digits 0..9\r\n",
    "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\r\n",
    "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\r\n",
    "        \r\n",
    "        # split up all addition problems into either training data or test data\r\n",
    "        num = (10**self.ndigit)**2 # total number of possible combinations\r\n",
    "        r = np.random.RandomState(1337) # make deterministic\r\n",
    "        perm = r.permutation(num)\r\n",
    "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\r\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return self.ixes.size\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        # given a problem index idx, first recover the associated a + b\r\n",
    "        idx = self.ixes[idx]\r\n",
    "        nd = 10**self.ndigit\r\n",
    "        a = idx // nd\r\n",
    "        b = idx %  nd\r\n",
    "        c = a + b\r\n",
    "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \r\n",
    "        print(render)\r\n",
    "        dix = [int(s) for s in render] # convert each character to its token index\r\n",
    "        # x will be input to GPT and y will be the associated expected outputs\r\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\r\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\r\n",
    "        print(x)\r\n",
    "        print(y)\r\n",
    "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\r\n",
    "        print(y)\r\n",
    "        exit()\r\n",
    "        return x, y\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# create a dataset for e.g. 2-digit addition\r\n",
    "ndigit = 2\r\n",
    "train_dataset = AdditionDataset(ndigit=ndigit, split='train')\r\n",
    "test_dataset = AdditionDataset(ndigit=ndigit, split='test')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_dataset[0] # sample a training instance just to see what one raw example looks like"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4717064\n",
      "tensor([4, 7, 1, 7, 0, 6])\n",
      "tensor([7, 1, 7, 0, 6, 4])\n",
      "tensor([-100, -100, -100,    0,    6,    4])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100,    0,    6,    4]))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from mingpt.model import GPT, GPTConfig, GPT1Config\r\n",
    "\r\n",
    "# initialize a baby GPT model\r\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, \r\n",
    "                  n_layer=2, n_head=4, n_embd=128)\r\n",
    "model = GPT(mconf)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "09/06/2021 09:48:16 - INFO - mingpt.model -   number of parameters: 4.001280e+05\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\r\n",
    "\r\n",
    "# initialize a trainer instance and kick off training\r\n",
    "tconf = TrainerConfig(max_epochs=50, batch_size=512, learning_rate=6e-4,\r\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(ndigit+1),\r\n",
    "                      num_workers=0)\r\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\r\n",
    "trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# now let's give the trained model an addition exam\r\n",
    "from torch.utils.data.dataloader import DataLoader\r\n",
    "from mingpt.utils import sample\r\n",
    "\r\n",
    "def give_exam(dataset, batch_size=32, max_batches=-1):\r\n",
    "    \r\n",
    "    results = []\r\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\r\n",
    "    for b, (x, y) in enumerate(loader):\r\n",
    "        x = x.to(trainer.device)\r\n",
    "        d1d2 = x[:, :ndigit*2]\r\n",
    "        d1d2d3 = sample(model, d1d2, ndigit+1)\r\n",
    "        d3 = d1d2d3[:, -(ndigit+1):]\r\n",
    "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(trainer.device)\r\n",
    "        # decode the integers from individual digits\r\n",
    "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\r\n",
    "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\r\n",
    "        d3i_pred = (d3 * factors).sum(1)\r\n",
    "        d3i_gt = d1i + d2i\r\n",
    "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\r\n",
    "        for i in range(x.size(0)):\r\n",
    "            results.append(int(correct[i]))\r\n",
    "            judge = 'YEP!!!' if correct[i] else 'NOPE'\r\n",
    "            if not correct[i]:\r\n",
    "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\" \r\n",
    "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\r\n",
    "        \r\n",
    "        if max_batches >= 0 and b+1 >= max_batches:\r\n",
    "            break\r\n",
    "\r\n",
    "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (D:\\myAPP\\vscode_extension\\extensions\\ms-toolsai.jupyter-2021.8.1236758218\\out\\client\\extension.js:90:327199)",
      "at w.execute (D:\\myAPP\\vscode_extension\\extensions\\ms-toolsai.jupyter-2021.8.1236758218\\out\\client\\extension.js:90:326520)",
      "at w.start (D:\\myAPP\\vscode_extension\\extensions\\ms-toolsai.jupyter-2021.8.1236758218\\out\\client\\extension.js:90:322336)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (D:\\myAPP\\vscode_extension\\extensions\\ms-toolsai.jupyter-2021.8.1236758218\\out\\client\\extension.js:90:336863)",
      "at async t.CellExecutionQueue.start (D:\\myAPP\\vscode_extension\\extensions\\ms-toolsai.jupyter-2021.8.1236758218\\out\\client\\extension.js:90:336403)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# training set: how well did we memorize?\r\n",
    "give_exam(train_dataset, batch_size=1024, max_batches=10)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test set: how well did we generalize?\r\n",
    "give_exam(test_dataset, batch_size=1024, max_batches=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# well that's amusing... our model learned everything except 55 + 45"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "5166982972f76bbf23080ab8866a42c3ebd4a79198991d025214334cacb9626c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}